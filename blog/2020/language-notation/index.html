<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Language of Notation and Imaginative Writing | Collin Jennings </title> <meta name="author" content="Collin Jennings"> <meta name="description" content="#A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon-16x16.png?77157cc524ac2722533433220437f63e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://collinjennings.com/blog/2020/language-notation/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Collin</span> Jennings </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/enlightenmentlinks/">Enlightenment Links Code </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Language of Notation and Imaginative Writing</h1> <p class="post-meta"> Created in July 22, 2020 </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/tag/language"> <i class="fa-solid fa-hashtag fa-sm"></i> language</a>   <a href="/blog/tag/vectors"> <i class="fa-solid fa-hashtag fa-sm"></i> vectors</a>   ·   <a href="/blog/category/blog"> <i class="fa-solid fa-tag fa-sm"></i> blog</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>What are the conditions under which very different words are brought together in writing? Are varied word combinations predisposed to particular genres or discourses? Are there types of words that could be said to constitute lexical situations that would not otherwise occur? And, when words that are not typically used in the same context co-occur, what are they doing? I am going to report the results of a text analysis experiment designed to begin to address these questions. Recent advances in semantic modeling (from topic modeling to word embeddings) make it relatively easy to describe the statistical likelihood of a given set of words to co-occur. Methods in semantic modeling start from the premises that words tend to occur in particular linguistic contexts, and we can decipher the meaning of an unknown word based upon the words that appear near it. While many humanists (like me) interested in text analysis have begun to explore the mathematics of operationalizing these premises, corpus linguists have been thinking about modeling language in this way since the 1940s and 50s. Early theorist of the “distributional structure” of language, Zellig Harris, explains that, “The perennial man in the street believes that when he speaks he freely puts together whatever elements have the meanings he intends; but he does so only by choosing members of those classes that regularly occur together and in the order in which these classes occur.”<a href="#fn0"><sup>1</sup></a><a href="#fn0"></a> His contemporary, J. R. Firth, put the claim even more plainly in his now famous formulation: “You shall know a word by the company it keeps.”<a href="#fn1"><sup>2</sup></a><a href="#fn1"></a> Since these foundational theories, linguists have produced methods for mathematically representing the tendencies of language. In this experiment, I focus on word space models, which represent the distribution of words in a corpus as vectors with hundreds or thousands of dimensions wherein proximity in the vector space corresponds to semantic similarity. The vector position for any given word represents a probabilistic profile regarding the lexical contexts in which one would expect to find that word. As a consequence, word vectors can be added and subtracted to find the words most similar in the model to the resulting composite vector.</p> <p><em>Two different semantic contexts of ‘bounds’</em> - The values appearing next to the resulting words refer to the cosine similarity score between the composite vector (produced by adding the positive word vectors and subtracting the negative one) and the most similar word vectors in the word space model. The score indicates the proximity of the vectors in the model, which should correspond to their semantic similarity—the closer to 1, the more similar words.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">bounds</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">leaps</span><span class="sh">'</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">limits</span><span class="sh">'</span><span class="p">])[:</span><span class="mi">6</span><span class="p">])</span>
<span class="nf">print</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">bounds</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">limits</span><span class="sh">'</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">leaps</span><span class="sh">'</span><span class="p">])[:</span><span class="mi">6</span><span class="p">])</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[('skips', 0.5588804483413696), ('leap', 0.5247325897216797), ('leapt', 0.5211300253868103), ('skipping', 0.5187863111495972), ('flings', 0.5138592720031738), ('alights', 0.5134779810905457)]

[('boundaries', 0.6185899376869202), ('limited', 0.595120906829834), ('limit', 0.5744596719741821), ('precincts', 0.5576064586639404), ('confined', 0.5283891558647156), ('confine', 0.5277968645095825)]
</code></pre></div></div> <p>Any particular phrase or sentence will feature word combinations that more or less correspond to linguistic patterns represented in the word space model. Defining expectation in terms of the proximity between word vectors, what are the contexts in which unexpected words tend to occur? Do they appear in particular places, like those rare neighborhood bars that attract patrons across a wide demographic range? Or are aberrational word groupings brought together by specific terms, like Prince or Beyoncé drawing fans from all backgrounds?</p> <p>I come to semantic modeling from a literary perspective. I initially started to think about measures for unusual linguistic situations as potentially indexing the appearance of literary tropes and figures. It’s a simplistic hypothesis, but it still seems reasonable to expect that unusual word combinations may be responsible for literal and figurative dimensions of meaning. The term <em>heart</em> often appears in discussions of biology as well as love; thus it may also appear in a context wherein both connotations obtain. Literary scholars have recently used semantic modeling to observe genre differences between fiction, poetry, and non-fiction as well as between sub-genres of the novel or particular forms of poetry during different historical periods.<a href="#fn2"><sup>3</sup></a><a href="#fn2"></a> This experiment draws upon this previous work but frames the critical questions both more narrowly and broadly in particular ways. On the one hand, I’m looking at the co-occurrence of words on the level of the sentence. Given the distribution of words across a large corpus, how likely are the words in a given sentence to co-occur? On the other hand, I’m considering all of the sentences in a corpus (rather than focusing on specific genres) in order to move from mathematical properties of word distributions to observations about how those properties manifest as linguistic functions in different discursive contexts. From this perspective, it is not at all clear that literary texts would stand out from a broader print environment in the ways we might expect. Literary, or imaginative, writing may even appear more formulaic and orderly than written forms that rely on direct, empirical observation. (I’ll be primarily using <em>imaginative writing</em> in place of the term <em>literature</em> since the latter is anachronistic for the seventeenth-century corpus used in this experiment.) According to the literary theorist Roland Barthes, the challenge for the novel writer, for one, concerns, “how to pass from Notation, and so from the Note, to the Novel, from the discontinuous to the flowing (to the continuous, the smooth [au nappé])?”<a href="#fn3"><sup>4</sup></a><a href="#fn3"></a> Notation here refers to annotating one’s experience—recording a series of events and thoughts that appear together without the linguistic architecture that gives language the statistical character that Harris and Firth describe. Translating the discontinuous record of consciousness into linear prose means moving “from the fragment to the nonfragment” (18).</p> <p>As the title of this post suggests, notation turns out to be very useful for explaining how the most unlikely linguistic situations are produced and organized in my corpus. I’m working with the publicly released portion of the <a href="http://www.textcreationpartnership.org/tcp-eebo/" rel="external nofollow noopener" target="_blank">Early English Books Online Text-Creation Partnership (EEBO-TCP)</a>, which consists of about 25,000 texts printed between 1475 and 1700. For this corpus, the preliminary answer to my opening string of questions is: there are particular terms that appear in the context of highly varied lexical contexts much more frequently than others, and these tend to be notational terms operating in a range of technical discourses (including heraldry, medicine, ornithology, and botany among others). Notation often refers to specific systems of signs and symbols used within a discipline like mathematics or music, but it can also be used in the manner that Barthes uses it above, to mean note-taking or annotating more generally. Most of the terms and contexts that I will present are best characterized by the first definition, but I will be using notation broadly to encompass both possible meanings because they each refer to processes of collecting unlike things—quantities, ideas, or observations—producing what Barthes calls “a layered text, a histology of cutups, a palimpsest.”<a href="#fn4"><sup>5</sup></a><a href="#fn4"></a> In seventeenth-century English print, the language of notation performs this function by serving as a linguistic framework of arrangement and combination (as in a list of ingredients for a medical recipe) without requiring the grammatical entailments of linear prose. Considered from a literary historical perspective, the linguistic work of notation offers a useful point of contact for thinking about forms of abstraction and comparison, which produce the conditions under which disparate things are brought together in language. In the case of imaginative writing, we typically ascribe such tasks to tropes and figures, but critical theorists like Barthes and more recently literary historians, including Henry Turner, Elaine Freedgood, and Cannon Schmidt have drawn attention to the function of “technical, denotative, and literal” language in literary works.<a href="#fn5"><sup>6</sup></a><a href="#fn5"></a> As I unpack the results of this experiment, I will locate different forms of notation in relation to this critical conversation in order to highlight the imaginative functions of this mode of arranging language.</p> <p>I produced a 300-dimension word space model of a portion of the EEBO-TCP corpus (all of the texts printed between 1640 and 1700, from the English Civil Wars through the Restoration and Revolution of 1688, consisting of 18,752 texts) using the <a href="https://en.wikipedia.org/wiki/Word2vec" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">word2vec</code></a> package in python. (I will refer to this as my Restoration model.)<a href="#fn6"><sup>7</sup></a><a href="#fn6"></a> <code class="language-plaintext highlighter-rouge">Word2Vec</code> uses a “shallow” neural network to generate a predictive word distribution model, rather than a count-based model. This approach makes it more efficient and easier to train, and some computer scientists argue that predictive word space models perform better than count-based ones, but this is a highly contested argument.<a href="#fn7"><sup>8</sup></a><a href="#fn7"></a> The challenge of using a predictive model for dealing with historical semantics, however, is that the training process makes it difficult to track how a term’s vector profile changes over time. To deal with this issue, I’ve taken a five-year subset (from 1678-1682, during the period of the Popish Plot and Exclusion Crisis in England) of the corpus used to train the model, so that I can compare the usage of words in the subset to their vector profiles in the larger Restoration model. The experiment that I present here then considers which words, occurring in texts printed between 1678 and 1682, tend to appear in contexts featuring the most varied combinations of terms relative to the larger Restoration model. While the results only directly refer to the five-year window of the subset, they appear to indicate a pattern that holds for most of the 1670s and 80s, based on other early investigations.</p> <p>In order to identify these results, I developed an average standard deviation measure (in collaboration with <a href="http://modelingliteraryhistory.org/" rel="external nofollow noopener" target="_blank">Michael Gavin</a>) that takes every context window for a given term in the five-year subset and finds the standard deviation between each vector position across the 300 dimensions for each word vector that co-occurs in a particular context window. The measure returns the standard deviation of the word vectors in a context window of the term, and that measure is averaged across all of the context windows for the term. The average standard deviation value then provides a proxy for determining how varied the lexical contexts tend to be for each term in the subset corpus (normalized to remove rare terms).<a href="#fn8"><sup>9</sup></a><a href="#fn8"></a> I also explored a method that used the average cosine similarity scores between a key term and the words in its context windows, which represents how likely that particular term would appear within its different contexts. The standard deviation measure, however, represents the variation between all of the words of the context window alone. As a consequence, the top scoring terms for the measure are not necessarily words that appear in the widest range of contexts; instead, the top terms are words that tend to appear in the most uncommon contexts in the subset overall. These are the words that most frequently occur at the center of rare word sequences.</p> <p>To see what this looks like, here are the top 50 terms returned for the average standard deviation measure for the 1678-1682 subset corpus:</p> <h2 id="top-50-terms-for-average-standard-deviation-measure">Top 50 terms for average standard deviation measure</h2> <p><img src="/assets/top_words.png" alt="terms"></p> <p>Since I am using a seventeenth-century corpus that includes Latin, English, and French texts including non-standardized spelling, these results look a bit inscrutable at first. I’ve gone through the most common texts in which each term appears in the corpus, and here is a breakdown of the different kinds of words in the top 50 that makes it easier to observe similarities across the terms:</p> <h2 id="word-types-in-the-top-50-terms">Word Types in the Top 50 Terms</h2> <p><img src="/assets/pieChart.png" alt="pie"></p> <p>From these types, I can make some basic observations. First, the terms primarily come from different technical discourses, including medicine, natural philosophy/alchemy, and cooking/agriculture. These tend to be learned contexts featuring a high prevalence of abstruse language. In part because of the association with expert communities, the terms appear in genres that often combine English and Latin. While I was initially tempted to dismiss Latin terms because they appear in a foreign language context (and thus will unavoidably seem more varied in relation to a primarily English corpus), the Latin words are often performing a similar semantic function to the English terms on the list. Of course, the foreign language context is still a factor here, but I will suggest below that it’s not sufficient to explain the placement of the top terms on the list. What’s more is that it’s not practical to simply remove the Latin texts from the corpus because most of the terms are coming from genres that contain Latin and English in the same context, such as botany and ornithology. An unavoidable feature of seventeenth-century English print is that it is a multilingual environment.</p> <h2 id="cluster-of-top-150-terms-appearing-in-the-most-varied-contexts">Cluster of Top 150 Terms appearing in the Most Varied Contexts</h2> <p>To gain more context regarding the sources of the terms, I performed K-means cluster analysis on all of the context windows for the top 150 terms. This meant treating all of the context windows for each term as a single document and then seeing which of the ‘term documents’ are the most similar. This is not far off from the logic behind training a word vector model, and it similarly produces a result in which words that appear in similar contexts are located near one another and far from words appearing in dissimilar contexts. In the two-dimensional representation of the clusters, we can see that <em>bushel</em>, <em>bushels</em>, <em>wheat</em>, and <em>barly</em> are near one another in the upper-left corner, and <em>fraction</em>, <em>numerator</em>, and <em>denominator</em> are clustered in the bottom-right corner. In the <code class="language-plaintext highlighter-rouge">word2vec</code> model, <em>fraction</em> returns a similarity score of 0.81 with <em>numerator</em> and 0.82 with <em>denominator</em>, and <em>numerator</em> and <em>denominator</em> return a score of 0.94—all of which indicate very high semantic similarity between the terms and thus suggest that the k-means graph accurately reflects the proximity between terms in the larger word space model.<br> <img src="/assets/kmeansNotation.png" alt="top terms"></p> <p>In k-means clustering (as with many topic modeling algorithms), the user has to select the number of clusters, and here I settled on four after identifying the top texts in which the words appeared in the subset. There appear to be four loose discourses in which these terms primarily occur. The table below breaks down the contents of these discourses:</p> <table class="mbtablestyle"> <thead> <tr> <th style="text-align: center">Cluster</th> <th style="text-align: center">Discourse</th> <th>Exemplary Titles</th> <th style="text-align: center">Top Terms</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><strong>Green</strong></td> <td style="text-align: center">Latin texts primarily on botany and natural history</td> <td>Robert Morison, <em>Plantarum historiæ universalis Oxoniensis. Pars secunda seu herbarum distributio nova, per tabulas congnationis &amp; affinitatis ex libro naturæ observata &amp; detecta</em> (1680)</td> <td style="text-align: center">folio, sine, sit, major, species, minor, anno, sex, alias</td> </tr> <tr> <td style="text-align: center"> </td> <td style="text-align: center"> </td> <td>Sir Robert Sibbald, <em>Scotland illustrated, or, An essay of natural history in which are exquisitely displayed the nature of the country, the dispositions and manners of the inhabitants…and the manifold productions of nature in its three-fold kingdom, (viz.) vegetable, animal and mineral, dispersed throughout the northern part of Great Brittain</em> (1684)</td> <td style="text-align: center"> </td> </tr> <tr> <td style="text-align: center"><strong>Orange</strong></td> <td style="text-align: center">Orinthology</td> <td>John Ray, <em>The ornithology of Francis Willughby of Middleton in the county of Warwick Esq, fellow of the Royal Society in three books : wherein all the birds hitherto known, being reduced into a method sutable to their natures, are accurately described</em> (1678)</td> <td style="text-align: center">feathers, white, black, colour, middle, tips, dusky, exteriour, outmost</td> </tr> <tr> <td style="text-align: center"> </td> <td style="text-align: center"> </td> <td>John Josselyn, <em>New-Englands rarities discovered in birds, beasts, fishes, serpents, and plants of that country</em> (1672)</td> <td style="text-align: center"> </td> </tr> <tr> <td style="text-align: center"><strong>Pink</strong></td> <td style="text-align: center">Heraldry</td> <td>Sir George Mackenzie, <em>The science of herauldry, treated as a part of the civil law, and law of nations wherein reasons are given for its principles, and etymologies for its harder terms.</em> (1680)</td> <td style="text-align: center">three, argent, betwixt, azur, sable, bend, within, beareth, quartered</td> </tr> <tr> <td style="text-align: center"> </td> <td style="text-align: center"> </td> <td>Robert Thoroton, <em>The antiquities of Nottinghamshire extracted out of records, original evidences, leiger books, other manuscripts, and authentick authorities : beautified with maps, prospects, and portraictures</em> (1677)</td> <td style="text-align: center"> </td> </tr> <tr> <td style="text-align: center"><strong>Purple</strong></td> <td style="text-align: center">Chemistry, medicine, and recipe books</td> <td>Moses Charras, <em>The Royal Pharmacopoea; Galenical and Chemical, according to the practice of the the most eminent and learned physicians of France</em> (1678)</td> <td style="text-align: center">half, ounce, two, dram, three, iij, bushels, ounces</td> </tr> <tr> <td style="text-align: center"> </td> <td style="text-align: center"> </td> <td>George Hartman, <em>The true preserver and restorer of health being a choice collection of select and experienced remedies for all distempers incident to men, women, and children: selected from and experienced by the most famous physicians and chyrurgeons in Europe: together with Excellent directions for cookery</em> (1682)</td> <td style="text-align: center"> </td> </tr> </tbody> </table> <p><br> Above I observed the prevalence of terms from technical discourses, and here we can see the kinds of texts in which these discussions occur. The titles indicate writers attempting to share different bodies of knowledge using specific taxonomies and systems of notation, as illustrated in the “science of herauldry” and an “essay of natural history.” The taxonomies are often attributed to a particular person or group, which creates the need to translate an individual system into the terms of a larger domain of shared knowledge. The top terms indexing the most varied contexts for each discourse include adjectives and numbers, but the most common type of terms, appearing across the clusters, are ones that seem to function as common ligatures used in descriptions of different cases and examples positioned within the respective taxonomies and systems. In ornithology, all species of birds have an <em>exterior</em>, <em>feathers</em>, and <em>tips</em> with features that are <em>outmost</em> or in the <em>middle</em>. Likewise, in natural histories written in Latin, different species are arranged in hierarchical structures with <em>major</em> and <em>minor</em> divisions. The purple cluster features a large amount of mathematical (<em>fraction</em>, <em>denominator</em>) and natural philosophical (<em>vitriolated</em>, <em>pulverized</em>) terms but also a significant number of units of measure and symbols (<em>ounces</em>, <em>drams</em>, <em>gallons</em>, <em>iij</em>, <em>aq</em>). While the latter terms belong to specific notational systems, the former also tend to appear in notational contexts.</p> <p>Turn to particular passages with high frequencies of notational terms, and one can observe the linguistic and epistemological functions that they perform to make highly varied contexts possible. Notational terms provide the linguistic mortar for producing technical conventions—the descriptors, transitions, and abstract units that recur in lists and tables of various early modern knowledge domains. Take the following recipe for a Powder of Crabs-Claws from <em>The Royal Pharmacopoea</em> (1678). The recipe is set apart from the main text as a list, written in Latin and English in side-by-side columns, with each unit of the ingredients expressed in various apothecary symbols. Although in typical conversation or writing a seventeenth-century Briton would be unlikely to mention <em>river crabs-eyes</em>, <em>white amber</em>, <em>deer’s heart-bone</em>, and <em>saffron</em> in the same context, the genre of the recipe and the notational format that accommodates it make these very unlike objects linguistically and literally combinable.</p> <p><img src="/assets/crabRecipe.png" alt="Crab claw powder screenshot" title="Moyse Charas, *The Royal Pharmacopoea*, (London: 1678)"></p> <p>In a radically different context, we can observe key terms from heraldry performing a similar function in the description of various coats of arms. Like the recipe, the table of noblemen and their coats of arms is represented in a distinctive printed form—set apart and arranged in a orderly manner, and, as in the recipe, there are particular recurring terms indicating the similarities across various coats even if the contents of the individual coats would be unlikely to appear together in a different context. These terms describe common colors and forms: for instance, <em>arg.</em> (the abbreviation for <em>argent</em> meaning <em>silver</em>), <em>passant</em> (the term for animals depicted in a walking position), and <em>azure</em>, as well as other terms, such as <em>betwixt</em> and <em>within</em>, indicate common prepositions used to link the different components of the coats.</p> <p><img src="/assets/coat_table.png" alt="Coat of arms table" title="Thomas Fuller, *The history of the worthies of England who for parts and learning have been eminent in the several counties : together with an historical narrative of the native commodities and rarities in each county*, (London: 1662)"></p> <p>Across disciplinary contexts, notation operates as a form of writing for translating subjective, potentially idiosyncratic observations into shared, intelligible systems of arrangement. Previous discussions of notation from a literary theoretical vantage have emphasized the “reality effect”<a href="#fn9"><sup>10</sup></a><a href="#fn9"></a> produced by including denotative details in fictional contexts, and, more recently, Elaine Freedgood, Cannon Schmitt, and others have explored the potential for “literalist” readings of denotative language in literary texts.<a href="#fn10"><sup>11</sup></a><a href="#fn10"></a> A literalist reading relies on tracking down references to outside knowledge domains, such as discussions of coal mining in Emile Zola’s <em>Germinal</em> or scientific language in George Elliot’s novels. This approach recognizes that insofar as novelists and poets represent characters participating in practices and crafts of the world, they unavoidably and intentionally introduce disciplines foreign from the experience of the reader. Like systems of notation, literary genres bring together disparate ideas and things. In highlighting notational language in non-literary contexts, I’m not suggesting that one should read recipes, blazons, or scientific tables as literary texts. Instead, I’m attempting to point out that notational contexts perform literary functions of combination, abstraction, and comparison, and recognizing such functions suggests new ways forward in describing how literary tropes and figures operate in relation to the layered format of notational lists and charts.</p> <p>Consider my favorite exemplary passage of notational language, a recipe for a horse-hoof unguent that appears in a 1686 hunting manual, called <em>The Gentleman’s Recreation</em>. <img src="/assets/horse_unguent.png" alt="passage" title="Nicholas Cox, The Gentleman's Recreation, (London: 1686)"></p> <p>The list of ingredients includes items that are familiar to us but would have been relatively novel to seventeenth-century readers (such as <em>turpentine</em>, which seems to have grown in popularity in the period) as well as objects completely foreign to us but very popular in the period (such as <em>trayn oil</em>). (Not to mention ones that are just baffling like <em>dog’s grease</em>, which turns out to be exactly what it sounds like.) These distinctive relationships between the objects in the recipe and our historical perspective indicate the varied meanings of the objects, beyond semantic significance. An item like train oil, which was produced by harvesting the fat from whale or cod, registers multiple dimensions of meaning in social and political domains in addition to linguistic ones. Indeed, historian K. G. Davies reports that in the 1680s, the Dutch government sent almost 2,000 whaling ships to Greenland to catch 10,000 whales for the production of train oil.<a href="#fn11"><sup>12</sup></a><a href="#fn11"></a> The combination of highly varied words in this example supports the idea that the lexical variation accommodated by notational contexts corresponds to related non-linguistic features—so-called real world consequences. The recipe bears out Barthes’s view of notation as “palimpsest,” wherein multiple semantic domains correspond to multiple historical temporalities. The constellation of disparate ingredients brings together the cultural, social, economic, and colonial histories of early modern Europe. At the moment of composition, the recipe conveyed a technique for creating a new substance out of many different ones, but as a historical document it represents a unique juncture, indicating something that could only have been conceived at a specific moment in time.</p> <p>The larger point is that the linguistic work of gathering together very different words indexes many more types of work that extend beyond the page and are implied by the histories of the objects and systems of notation represented. This linguistic phenomenon represents one answer to the question posed by Bruno Latour, “<em>how do we pack the world into words?</em>”<a href="#fn12"><sup>13</sup></a><a href="#fn12"></a> From a literary studies angle, I would slightly reframe the question to ask, how does imaginative writing pack the world into words similarly or differently from other modes and genres? In a discussion of mathematical thinking and the imagination, Arielle Saiber and Henry Turner posit that “imagination is that faculty of thinking that facilitates movement across systems of explanation that seem irreconcilable, and that, as a consequence, allows for new thoughts, new arguments, and new explanations to occur.”<a href="#fn13"><sup>14</sup></a><a href="#fn13"></a> If this is the case, then what are the linguistic structures and tropes that make this movement possible? This experiment regarding lexical variation and word space models illustrates how the language of notation contributes to such movement. How might we identify other contributors and the imaginative work they accomplish?</p> <h2 id="references">References</h2> <p><a id="fn0"></a>    1. Zellig Harris, “Distributional Structure,” <em>The Structure of Language: Readings in the Philosophy of Language</em>, ed. Jerry A. Fodor and Jerrold J. Katz (Englewood Cliff, NJ: Prentice-Hall, 1964), 34.</p> <p><a id="fn1"></a>     2. J. R. Firth, <em>Papers in Linguistics,1934–1951</em> (London: Oxford University Press, 1957), 11.</p> <p><a id="fn2"></a>     3. Roland Barthes, <em>The Preparation of the Novel: Lecture Courses and Seminars at the Collège de France</em> [2003], 3rd ed. (New York: Columbia University Press, 2010), 18. Also for an extended analysis of how Barthes’s understanding of notation changed from “The Reality Effect” essay to The Preparation of the Novel lectures, see Rachel Sagner Buurma and Lauren Heffernan, “Notation After ‘The Reality Effect’: Remaking Reference with Roland Barthes and Sheila Heti,” <em>Representations</em> 125.1 (Winter, 2014), 80-102. This essay appears in the fantastic special issue of <em>Representations</em> on denotative and technical language in the novel. I cite the introduction below.</p> <p><a id="fn3"></a>     4. There’s of course a lot of great recent critical work using semantic modeling. For topic modeling, the <a href="http://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/" rel="external nofollow noopener" target="_blank"><em>Journal of the Digital Humanities</em></a> 2.1 (Winter, 2012), special issue on the subject is a great place to start. For thinking about, linguistic patterns on the scale of the sentence, see Sarah Allison, et al., <a href="https://litlab.stanford.edu/LiteraryLabPamphlet5.pdf" rel="external nofollow noopener" target="_blank">“Pamphlet 5: Style at the Scale of the Sentence”</a>,” (June 2013) <em>Lit Lab</em>. Useful introductions to word embeddings include Lynn Cherney’s site word embedding version of “<a href="http://www.ghostweather.com/files/word2vecpride/" rel="external nofollow noopener" target="_blank"><em>Pride and Prejudice</em> and Word Embedding Distance</a>”; Michael Gavin, <a href="http://modelingliteraryhistory.org/2015/09/18/the-arithmetic-of-concepts-a-response-to-peter-de-bolla/" rel="external nofollow noopener" target="_blank">“The Arithmetic of Concepts: A Response to Peter de Bolla”</a>, Sept. 18, 2015; Ben Schmidt, <a href="http://bookworm.benschmidt.org/posts/2015-10-25-Word-Embeddings.html" rel="external nofollow noopener" target="_blank">“Vector Space Models for the Digital Humanities”</a>, Oct. 25, 2015.</p> <p><a id="fn4"></a>     5. Barthes, 18.</p> <p><a id="fn5"></a>     6. See Elaine Freedgood and Cannon Schmitt, “Denotatively, Technically, Literally,” <em>Representations</em> 125.1 (Winter, 2014), 1-14; and Henry Turner, <em>The English Renaissance Stage: Geometry, Poetics, and the Practical Spatial Arts, 1580-1630</em> (Oxford: Oxford University Press, 2006), 114-154.</p> <p><a id="fn6"></a>    7. The model was trained on context windows spanning 5 words on either side of a given term, for words appearing at least 40 times in the corpus. I removed stopwords (using the standard nltk stopword list), but I did not stem or lemmatize the corpus. The model and a csv of the model contents can be downloaded <a href="https://www.dropbox.com/sh/vg0v4evj3ru4ziz/AAA60zxXvN_NYTMtDzmNx508a?dl=0" rel="external nofollow noopener" target="_blank">here</a>. Since I created the model, the Visualizing English Print team has publicly released a <a href="http://graphics.cs.wisc.edu/WP/vep/tcp/" rel="external nofollow noopener" target="_blank">version</a> of the EEBO-TCP corpus with standardized spelling, which, I expect, would produce a more accurate model for word sense disambiguation.</p> <p><a id="fn7"></a>    8. For arguments supporting count-based approaches to semantic modeling, see Omer Levy, et al., “Improved Distributional Similarity with Lessons Learned from Word Embeddings,” <em>Transactions of the Association for Computational Linguistics</em> 3 (2015): 211-225; Omer Levy and Yoav Goldberg, “Linguistic Regularities in Sparse and Explicit Word Representations,” <em>Proceedings of the Eighteenth Conference on Computational Language Learning</em> (2014): 171-180. For the rationale behind predictive modeling, see Marco Baroni, et al., “Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors,” <em>Proceedings of Association for Computational Linguistics</em> (2014); as well as the paper that introduced the <code class="language-plaintext highlighter-rouge">word2vec</code> algorithm, Tomas Mikolov, et al., “Efficient Estimation of Word Representations in Vector Space,” (2013), <a href="http://arxiv.org/abs/1301.3781" rel="external nofollow noopener" target="_blank">http://arxiv.org/abs/1301.3781</a>.</p> <p><a id="fn8"></a>     9. There is still a lot work to do in using word distribution models to analyze phrases and sentences. <code class="language-plaintext highlighter-rouge">Word2vec</code> has been used to identify words that tend to appear in phrases. See Mikolov, et al., <a href="https://arxiv.org/abs/1310.4546" rel="external nofollow noopener" target="_blank">“Distributed Representations of Words and Phrases and their Compositionality,”</a> (Oct., 2013). Yet I have not seen persuasive results in using such models to analyze sentence similarity. My own attempts to use <code class="language-plaintext highlighter-rouge">word2vec</code> to analyze sentence similarity in the EEBO-TCP corpus has not worked at all. My standard deviation measure represents a somewhat crude but effective method for looking at the semantic similarities between words in context windows. I’m sure that there will be additional approaches that might be more useful in the future. Here’s just some information on results from using the measure in this experiment. The mean standard deviation value for each word in the subset 1678-1682 corpus (averaged across all of the context windows of each word) is <code class="language-plaintext highlighter-rouge">0.11229</code>. The mean for the top 150 terms that I present here is <code class="language-plaintext highlighter-rouge">0.139412</code>, which is a little more than five standard deviations above the mean. While these terms stand out from the rest, it is of course true that there is only a relative difference between the lexical variation appearing in the context windows of the top terms and those of the rest of the words in the model. Above I focus on the contexts of the top terms, but the terms with the lowest values for the measure also indicate its efficacy. For instance two of the bottom terms are <em>oge</em> and <em>ordeining</em>, which like many of the top terms are difficult to contextualize at first glance. In the subset, <em>Oge</em> primarily occurs in Irish genealogies in which formulaic phrases appear with great frequency, and the resulting context windows contain a lot of repeated words. Similarly, <em>ordeining</em> largely appears in formulaic phrases in religious texts, most notably <em>the power of ordeining others</em>. For a prime example, see Henry Dodwell, <a href="http://quod.lib.umich.edu/e/eebo/A36253.0001.001/1:29?amt2=120;amt3=40;c=eebo;c=eebo2;g=eebogroup;rgn=div1;view=toc;xc=1;q1=ordeining;op2=near;q2=others;op3=near;q3=power" rel="external nofollow noopener" target="_blank"><em>Separation of churches from episcopal government, as practised by the present non-conformists, proved schismatical from such principles as are least controverted and do withal most popularly explain the sinfulness and mischief of schism</em></a> (London: 1679). Finally, using my subsetting method produces the additional issue of reducing the size of the corpus under analysis and thus allowing texts with idiosyncratic linguistic structures to dominate. The subset corpus contains 4,370 texts. I think working with count-based distribution models might make this problem easier to avoid, although it may introduce other challenges regarding the size of the model.</p> <p><a id="fn9"></a>     10. Roland Barthes, “The Reality Effect,” <em>The Rustle of Language</em> (Berkeley and Los Angeles: University of California, 1989), 141.</p> <p><a id="fn10"></a>     11. Freedgood and Schmitt, 5.</p> <p><a id="fn11"></a>     12. K. G. Davies, <em>The North Atlantic World in the Seventeenth Century</em> (Minneapolis: University of Minnesota Press, 1974), 157.</p> <p><a id="fn12"></a>     13. Bruno Latour, “Circulating Reference: Sampling the Soil in the Amazon Forest,” <em>Pandora’s Hope: Essays on the Reality of Science Studies</em> (Cambridge: Harvard University Press, 1999), 24.</p> <p><a id="fn13"></a>     14. Arielle Seiber and Henry Turner, “Mathematics and the Imagination: A Brief Introduction,” <em>Configurations</em> 17 (2009), 12.</p> <table style="border-style: hidden; border-collapse: collapse;"> </table> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/aeonArticle/">Aeon Article - A Linkless Internet</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/detectiveExperimentPost-1/">The Frame of Reference: Solving Detective Stories with AI - #1</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/history-literature/">The History of Literature Podcast - Mind and Media in the Enlightenment</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/stats-stories/">Stats + Stories Podcast - "Judging Words by the Company They Keep"</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2016/MoMADaTA/">Critical Potluck - MoMA DaTA</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Collin Jennings. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-enlightenment-links-code",title:"Enlightenment Links Code",description:"",section:"Navigation",handler:()=>{window.location.href="/enlightenmentlinks/"}},{id:"post-aeon-article-a-linkless-internet",title:"Aeon Article - A Linkless Internet",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/aeonArticle/"}},{id:"post-the-frame-of-reference-solving-detective-stories-with-ai-1",title:"The Frame of Reference: Solving Detective Stories with AI - #1",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/detectiveExperimentPost-1/"}},{id:"post-the-history-of-literature-podcast-mind-and-media-in-the-enlightenment",title:"The History of Literature Podcast - Mind and Media in the Enlightenment",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/history-literature/"}},{id:"post-stats-stories-podcast-quot-judging-words-by-the-company-they-keep-quot",title:"Stats + Stories Podcast - &quot;Judging Words by the Company They Keep&quot;",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/stats-stories/"}},{id:"post-the-language-of-notation-and-imaginative-writing",title:"The Language of Notation and Imaginative Writing",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/language-notation/"}},{id:"post-critical-potluck-moma-data",title:"Critical Potluck - MoMA DaTA",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/MoMADaTA/"}},{id:"post-privacy-performed-at-scale-a-critical-potluck",title:"Privacy Performed at Scale - A Critical Potluck",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/privacy-performed/"}},{id:"post-mining-your-search-results",title:"Mining Your Search Results",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/mining-search/"}},{id:"news-excited-to-have-the-chance-to-discuss-39-enlightenment-links-39-with-jacke-on-39-the-history-of-literature-39-podcast-https-www-historyofliterature-com-649-mind-and-media-in-the-enlightenment-with-collin-jennings-mike-recommends-a-moveable-feast-by-check-it-out",title:"Excited to have the chance to discuss [&#39;Enlightenment Links&#39; with Jacke on &#39;The...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%63%6F%6C%6C%69%6E.%6A%65%6E%6E%69%6E%67%73@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/collinjennings","_blank")}},{id:"socials-bluesky",title:"Bluesky",section:"Socials",handler:()=>{window.open("https://bsky.app/profile/https://bsky.app/profile/collinjennings.bsky.social","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>